{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "from nltk.corpus import TwitterCorpusReader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import pickle\n",
    "from gaft import GAEngine\n",
    "from gaft.components import BinaryIndividual\n",
    "from gaft.components import Population\n",
    "from gaft.operators import TournamentSelection\n",
    "from gaft.operators import UniformCrossover\n",
    "from gaft.operators import FlipBitMutation\n",
    "\n",
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analysis plugin base class.\n",
    "from gaft.plugin_interfaces.analysis import OnTheFlyAnalysis\n",
    "\n",
    "# Built-in best fitness analysis.\n",
    "from gaft.analysis.fitness_store import FitnessStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data for training\n",
    "#tweet data source (please follow https://www.nltk.org/data.html to download data)\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "#load\n",
    "neg_tweet = twitter_samples.strings(fileids = 'negative_tweets.json')\n",
    "pos_tweet = twitter_samples.strings(fileids = 'positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweets: list of strings ; lemmatizer: nltk Lemmatizer ; stemmer : nltk Stemmer\n",
    "#remove stopword and tokenize strings. lemmatize/stemming if lemmatizer/stemmer not None\n",
    "def preprocessString(tweets, lemmatizer, stemmer):\n",
    "    \n",
    "    #stopword\n",
    "    #tweets = [tweet.lower() for tweet in tweets if tweet.lower() not in stopwords.words('english')]\n",
    "    #tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweets = [tokenizer.tokenize(tweet) for tweet in tweets]\n",
    "    #lower\n",
    "    for i in range(len(tweets)):\n",
    "        tweets[i] = [w.lower() for w in tweets[i] if ((w not in stopwords.words('english')) & w.isalpha())]\n",
    "    \n",
    "    if lemmatizer != None:\n",
    "        for i in range(len(tweets)):\n",
    "            #lemmatization \n",
    "            tweets[i] = [lemmatizer.lemmatize(t) for t in tweets[i]]\n",
    "    if stemmer != None:\n",
    "        for i in range(len(tweets)):\n",
    "            #stemming \n",
    "            tweets[i] = [stemmer.stem(t) for t in tweets[i]]\n",
    "    \n",
    "    #Collocations, Bigrams, Trigrams\n",
    "    #Chunking\n",
    "            \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "#stemming\n",
    "pstemmer = nltk.PorterStemmer()\n",
    "\n",
    "#process tweets\n",
    "neg_tweetPro = preprocessString(neg_tweet, wnl, pstemmer)\n",
    "pos_tweetPro = preprocessString(pos_tweet, wnl, pstemmer)\n",
    "docs = neg_tweetPro + pos_tweetPro\n",
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorizer\n",
    "def identity(x):\n",
    "    return x\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(preprocessor=identity, tokenizer=identity)\n",
    "docsX = vectorizer.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define population.\n",
    "numWords = len(words)\n",
    "outputWordLimit = 5\n",
    "rngs =  [(0, 1)] * numWords  * outputWordLimit * 2 #weight and bias\n",
    "indv_template = BinaryIndividual(ranges=rngs, eps=0.01)\n",
    "population = Population(indv_template=indv_template, size=100).init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create genetic operators.\n",
    "selection = TournamentSelection(tournament_size=20)\n",
    "crossover = UniformCrossover(pc=0.8, pe=0.5)\n",
    "mutation = FlipBitMutation(pm=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create genetic algorithm engine.\n",
    "engine = GAEngine(population=population, selection=selection,\n",
    "                  crossover=crossover, mutation=mutation,\n",
    "                  analysis=[FitnessStore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@engine.analysis_register\n",
    "class ConsoleOutputAnalysis(OnTheFlyAnalysis):\n",
    "    interval = 1\n",
    "    master_only = True\n",
    "\n",
    "    def register_step(self, g, population, engine):\n",
    "        best_indv = population.best_indv(engine.fitness)\n",
    "        msg = 'Generation: {}, best fitness: {:.3f}'.format(g, engine.ori_fmax)\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    def finalize(self, population, engine):\n",
    "        best_indv = population.best_indv(engine.fitness)\n",
    "        x = best_indv.solution\n",
    "        y = engine.ori_fmax\n",
    "        msg = 'Optimal solution: ({}, {})'.format(x, y)\n",
    "        self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@engine.fitness_register\n",
    "def fitness(indv):\n",
    "    x = indv.solution\n",
    "    wordChoice = np.zeros(outputWordLimit)\n",
    "    for input_bag in docsX:\n",
    "        for o in range(outputWordLimit):\n",
    "            p_start = numWords * o * 2\n",
    "            paramsw = np.array(x[p_start:p_start + numWords])\n",
    "            paramsb = np.array(x[p_start + numWords: p_start + numWords * 2])\n",
    "            weight = input_bag * paramsw + paramsb\n",
    "            wordChoice[o] = np.argmax(weight)\n",
    "    sent = \" \".join([words[int(c)] for c in wordChoice])\n",
    "    errors = tool.check(sent)\n",
    "    return (10-min(10,len(errors)))/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.run(ng=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import language_check\n",
    "#dir(language_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
