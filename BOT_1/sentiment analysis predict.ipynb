{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import TwitterCorpusReader\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweets: list of strings ; lemmatizer: nltk Lemmatizer ; stemmer : nltk Stemmer\n",
    "#remove stopword and tokenize strings. lemmatize/stemming if lemmatizer/stemmer not None\n",
    "def preprocessString(tweets, lemmatizer, stemmer):\n",
    "    \n",
    "    #stopword\n",
    "    tweets = [w for w in tweets if w.lower() not in stopwords.words('english')]\n",
    "    #tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweets = [tokenizer.tokenize(tweet) for tweet in tweets]\n",
    "    #lower\n",
    "    for i in range(len(tweets)):\n",
    "        tweets[i] = [t.lower() for t in tweets[i]]\n",
    "    \n",
    "    if lemmatizer != None:\n",
    "        for i in range(len(tweets)):\n",
    "            #lemmatization \n",
    "            tweets[i] = [lemmatizer.lemmatize(t) for t in tweets[i]]\n",
    "    if stemmer != None:\n",
    "        for i in range(len(tweets)):\n",
    "            #stemming \n",
    "            tweets[i] = [stemmer.stem(t) for t in tweets[i]]\n",
    "    \n",
    "    #Collocations, Bigrams, Trigrams\n",
    "    #Chunking\n",
    "            \n",
    "    return tweets\n",
    "\n",
    "#tweets: list of objects, tag: tag in string \n",
    "def add_tags(tweets, tag):\n",
    "    return [[tweet, tag] for tweet in tweets]\n",
    "\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#StreamListener\n",
    "from secret import *\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(C_KEY, C_SECRET)  \n",
    "auth.set_access_token(A_TOKEN, A_TOKEN_SECRET)  \n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "#from tweepy API\n",
    "class TBStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def __init__(self,api=None):\n",
    "        super().__init__(api)\n",
    "        self.tweets = []\n",
    "        self.numTweet = 20\n",
    "\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.numTweet > 0:\n",
    "            data = json.loads(data)\n",
    "            self.tweets.append(data['text'])\n",
    "            self.numTweet -= 1\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            #returning False in on_error disconnects the stream\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grab tweet\n",
    "tbStreamListener = TBStreamListener()\n",
    "stream = tweepy.Stream(auth = api.auth, listener=tbStreamListener)\n",
    "\n",
    "#not both sample and filter\n",
    "#stream.sample(True, languages = ['en'])\n",
    "stream.filter(track=['twitter'], languages = ['en']) #, async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @chelseynvana: happy ass potato https://t.co/5YYX58rSdO',\n",
       " 'RT @NoahCRothman: WTF. \"Some Coral Springs police were stunned and upset that the four original Broward County Sheriff\\'s deputies who wereâ€¦',\n",
       " \"Here's the official ðŸ’¯âž• club of 13 strong from the testing results at Sunday's #TheOpening regional in Miami https://t.co/ETQb0yDb0K\",\n",
       " 'RT @snmrrw: Facebook is at CPAC and they have a VR shooting game https://t.co/wmV23jezpN',\n",
       " 'Starting to post some #STL photography from my archives.  #photography #photoshare #STLphotos  check outâ€¦ https://t.co/UGeALwJxMX',\n",
       " 'RT @OnlyHipHopFacts: 19 years ago today, Eminem released The Slim Shady LP. https://t.co/bs0qjbMrS4',\n",
       " \"RT @connor_hannigan: when you're full on chips &amp; salsa and your food arrives https://t.co/fa76MET61R\",\n",
       " 'Hey @FedEx - please join us and end your relationship with the @NRA. #BoycottNRA #NeverAgain #ArmMeWith https://t.co/sZEukedWI4',\n",
       " \"Can't forget this day when I enjoyed this gorgeous beauty while u sitting in front of us. She was so naughtyðŸ˜ˆ thankâ€¦ https://t.co/GkOEjsLkyU\",\n",
       " \"I'm with #KD, who ya got? https://t.co/gqcKqGs41L\",\n",
       " 'RT @Drebae_: Yâ€™all really be putting black kids in bad situations before they even old enough to make conscious decisions. I canâ€™t even beâ€¦',\n",
       " 'RT @Yankees: There was a lot to love about today.\\n\\n#PinstripePride https://t.co/KSGQIKrtPC',\n",
       " 'RT @USMC: 73 years ago today, Marines on Iwo Jima raised the flag atop Mount Suribachi.\\n\\nSemper Fidelis. https://t.co/lRUTwLp9fR',\n",
       " \"I'm with #Russ, who ya got? https://t.co/146SnHpIzr\",\n",
       " 'RT @YABOYLILB: What did I just witness https://t.co/ujOc1S0FZz',\n",
       " \"RT @aldouga: #Saumya is influenced by #Harman 's status and tells him that her life without him has stopped An impressive scene full of sufâ€¦\",\n",
       " \"RT @brhodes: Don't be distracted. Don't treat it as background noise. Don''t let it be normalized. Register. Organize. And vote out Republiâ€¦\",\n",
       " 'RT @rachelmonline: Be sure to stay until after the credits end when you are seeing #GameNght! Thereâ€™s a post credit scene according to oneâ€¦',\n",
       " 'RT @jeonss97: Jungkook did his iconic pose with his own matching sound effects I LOVE THIS CONCEPT SO MUCH SO ADORABLE BABY BUNNY \\n\\n#iHeartâ€¦',\n",
       " 'RT @prowlastator: nick roche is horny on PUBLIC CONTENT https://t.co/6omuwrGsch']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to wait for a while\n",
    "tbStreamListener.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "#from nltk.corpus import twitter_samples\n",
    "#tweet = twitter_samples.strings(fileids = 'negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet = tbStreamListener.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's the official ðŸ’¯âž• club of 13 strong from the testing results at Sunday's #TheOpening regional in Miami https://t.co/ETQb0yDb0K\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#lemmatization\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "#stemming\n",
    "pstemmer = nltk.PorterStemmer()\n",
    "#process tweets\n",
    "test_data = preprocessString(tweet, wnl, pstemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorizer\n",
    "vectorizer = pickle.load(open(\"tfidf_veczr.pkl\",\"rb\"))\n",
    "testX = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier\n",
    "classifier = pickle.load(open(\"SVM_clf.pkl\",\"rb\"))\n",
    "classifier.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
